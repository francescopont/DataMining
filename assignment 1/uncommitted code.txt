utilities.precision <- function (actual.values, predictions){
  matrix <- cbind(actual.values, predictions)
  return ((true.positives(matrix)/(true.positives(matrix) + false.positives(matrix))))
}


utilities.recall <- function (actual.values, predictions){
  matrix <- cbind(actual.values, predictions)
  return ((true.positives(matrix)/(true.positives(matrix) + false.negatives(matrix))))
}

utilities.accuracy <- function(actual.values, predictions){
  matrix <- cbind(actual.values, predictions)
  return (((true.negatives(matrix) + true.positives(matrix))/nrow(matrix)))
}

utilities.mcNemar <- function(before.pred, after.pred, actual.values){
  pp <- 0
  pn <- 0
  np <- 0
  nn <- 0
  before.pred <- sapply(c(1:length(before.pred)), function(i){
    if(before.pred[i] == actual.values[i]){
      return (1)
    }
    else return (0)
  })
  after.pred <- sapply(c(1:length(after.pred)), function(i){
    if(after.pred[i] == actual.values[i]){
      return (1)
    }
    else return (0)
  })
  for( i in c(1:length(before.pred))){
    before.value <- before.pred[i]
    after.value <- after.pred[i]
    if (before.value == 1 && after.value == 1){
      pp <- pp +1
    }
    if (before.value == 1 && after.value == 0){
      pn <- pn +1
    }
    if (before.value == 0 && after.value == 1){
      np <- np +1
    }
    if (before.value == 0 && after.value == 0){
      nn <- nn +1
    }
  }
  performance <-matrix(c(pp, np, pn, nn),nrow = 2,
           dimnames = list("single tree" = c("Positive", "Negative"),
                           "bagging" = c("Positive", "Negative")))
  mcnemar.test(performance)
}

true.positives <- function(matrix){
  true.positive <- apply(matrix, 1, function(row){
    if(row[1] ==1 && row[2] ==1){
      return (1)
    }
    else return (0)
  })
  return (sum(true.positive))
}

false.positives <- function(matrix){
  false.positive <- apply(matrix, 1, function(row){
    if(row[1] ==0 && row[2] ==1){
      return (1)
    }
    else return (0)
  })
  return (sum(false.positive))
}

true.negatives <- function(matrix){
  true.negative <- apply(matrix, 1, function(row){
    if(row[1] ==0 && row[2] ==0){
      return (1)
    }
    else return (0)
  })
  return (sum(true.negative))
}

false.negatives <- function(matrix){
  false.negative <- apply(matrix, 1, function(row){
    if(row[1] ==1 && row[2] ==0){
      return (1)
    }
    else return (0)
  })
  return (sum(false.negative))
}


#utilities.precision
###parameters###
#actual values: vector of actual class labels of the test set (0 or 1)
#predictions: vector of the predictions ( 0 or 1)
###returns###
#the precision of the classification model
utilities.precision <- function (actual.values, predictions){
  matrix <- cbind(actual.values, predictions)
  return ((true.positives(matrix)/(true.positives(matrix) + false.positives(matrix))))
}

#utilities.recall
###parameters###
#actual values: vector of actual class labels of the test set (0 or 1)
#predictions: vector of the predictions ( 0 or 1)
###returns###
#the recall of the classification model
utilities.recall <- function (actual.values, predictions){
  matrix <- cbind(actual.values, predictions)
  return ((true.positives(matrix)/(true.positives(matrix) + false.negatives(matrix))))
}
#utilities.accuracy
###parameters###
#actual values: vector of actual class labels of the test set (0 or 1)
#predictions: vector of the predictions ( 0 or 1)
###returns###
#the accuracy of the classification model
utilities.accuracy <- function(actual.values, predictions){
  matrix <- cbind(actual.values, predictions)
  return (((true.negatives(matrix) + true.positives(matrix))/nrow(matrix)))
}

#utilities.mcNemar
###parameters###
#before.pred: vector of binary predictions of the first model
#after.pred: vector of binary predictions of the second model
#actual.values:vector of actual class labels of the observations
###returns###
#nothing
#print the results of the McNemar test between the first and the second vector of predictions
#to find out statistically significant differences
utilities.mcNemar <- function(before.pred, after.pred, actual.values){
  pp <- 0 #number of obs that are correct in both predictions
  pn <- 0 #number of obs that are correct in the first prediction
  np <- 0 #number of obs that are correct in the second prediction
  nn <- 0 #number of obs that are incorrect in both predictions
  # we modify the vectors: 1 means that the prediction was right, 0 that it was wrong
  before.pred <- sapply(c(1:length(before.pred)), function(i){
    if(before.pred[i] == actual.values[i]){
      return (1)
    }
    else return (0)
  })
  after.pred <- sapply(c(1:length(after.pred)), function(i){
    if(after.pred[i] == actual.values[i]){
      return (1)
    }
    else return (0)
  })
  #computation of pp, pn, np, nn
  for( i in c(1:length(before.pred))){
    before.value <- before.pred[i]
    after.value <- after.pred[i]
    if (before.value == 1 && after.value == 1){
      pp <- pp +1
    }
    if (before.value == 1 && after.value == 0){
      pn <- pn +1
    }
    if (before.value == 0 && after.value == 1){
      np <- np +1
    }
    if (before.value == 0 && after.value == 0){
      nn <- nn +1
    }
  }
  performance <-matrix(c(pp, np, pn, nn),nrow = 2,
           dimnames = list("single tree" = c("Positive", "Negative"),
                           "bagging" = c("Positive", "Negative")))
  mcnemar.test(performance)
}

#true.positives
###parameters###
#matrix: a matrix containing the actual class labels as first column and the predictions as second column
###returns###
# the number of true positives (observations predicted as of class 1 and actually of class 1)
true.positives <- function(matrix){
  true.positive <- apply(matrix, 1, function(row){
    if(row[1] ==1 && row[2] ==1){
      return (1)
    }
    else return (0)
  })
  return (sum(true.positive))
}

#false.positives
###parameters###
#matrix: a matrix containing the actual class labels as first column and the predictions as second column
###returns###
# the number of false positives (observations predicted as of class 1 and actually of class 0)
false.positives <- function(matrix){
  false.positive <- apply(matrix, 1, function(row){
    if(row[1] ==0 && row[2] ==1){
      return (1)
    }
    else return (0)
  })
  return (sum(false.positive))
}

#true.negatives
###parameters###
#matrix: a matrix containing the actual class labels as first column and the predictions as second column
###returns###
# the number of true positives (observations predicted as of class 0 and actually of class 0)
true.negatives <- function(matrix){
  true.negative <- apply(matrix, 1, function(row){
    if(row[1] ==0 && row[2] ==0){
      return (1)
    }
    else return (0)
  })
  return (sum(true.negative))
}

#false.negatives
###parameters###
#matrix: a matrix containing the actual class labels as first column and the predictions as second column
###returns###
# the number of false positives (observations predicted as of class 0 and actually of class 1)
false.negatives <- function(matrix){
  false.negative <- apply(matrix, 1, function(row){
    if(row[1] ==1 && row[2] ==0){
      return (1)
    }
    else return (0)
  })
  return (sum(false.negative))
}
