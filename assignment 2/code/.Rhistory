source('~/GitHub/DataMining/assignment 2/code/cleaning function.R')
library(tm)
library(randomForest)
training.corpus.dec <- c("C:/Users/ponti/Documenti/Github/DataMining/assignment 2/op_spam_v1.4/negative_polarity/deceptive_from_MTurk/fold1"
,"C:/Users/ponti/Documenti/Github/DataMining/assignment 2/op_spam_v1.4/negative_polarity/deceptive_from_MTurk/fold2"
,"C:/Users/ponti/Documenti/Github/DataMining/assignment 2/op_spam_v1.4/negative_polarity/deceptive_from_MTurk/fold3"
,"C:/Users/ponti/Documenti/Github/DataMining/assignment 2/op_spam_v1.4/negative_polarity/deceptive_from_MTurk/fold4")
training.corpus.true<- c("C:/Users/ponti/Documenti/Github/DataMining/assignment 2/op_spam_v1.4/negative_polarity/truthful_from_Web/fold1"
,"C:/Users/ponti/Documenti/Github/DataMining/assignment 2/op_spam_v1.4/negative_polarity/truthful_from_Web/fold2"
,"C:/Users/ponti/Documenti/Github/DataMining/assignment 2/op_spam_v1.4/negative_polarity/truthful_from_Web/fold3"
,"C:/Users/ponti//Documenti/Github/DataMining/assignment 2/op_spam_v1.4/negative_polarity/truthful_from_Web/fold4")
testing.corpus.dec  <- "C:/Users/ponti/Documenti/Github/DataMining/assignment 2/op_spam_v1.4/negative_polarity/deceptive_from_MTurk/fold5"
testing.corpus.true <- "C:/Users/ponti/Documenti/Github/DataMining/assignment 2/op_spam_v1.4/negative_polarity/truthful_from_Web/fold5"
#training set
training.dtm <- cleaning.function(training.corpus.dec,training.corpus.true)
#extraction of unigrams
training.dtm.unigrams <- DocumentTermMatrix(training.dtm)
training.dtm.unigrams <- removeSparseTerms(training.dtm.unigrams,0.95)
training.dtm.unigrams <- as.matrix(training.dtm.unigrams)
#extraction of bigrams
BigramTokenizer <-function(x) unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
training.dtm.bigrams <- DocumentTermMatrix(training.dtm,control = list(tokenize = BigramTokenizer))
training.dtm.bigrams <- removeSparseTerms(training.dtm.bigrams,0.95)
training.dtm.bigrams <- as.matrix(training.dtm.bigrams)
#training set with both unigrams and bigrams
training.labels <- c(rep(0,320),rep(1,320))
training.labels <- as.factor(training.labels)
training.dtm <- cbind(training.dtm.unigrams, training.dtm.bigrams)
###################################################################################
#test set
test.dtm <- cleaning.function (testing.corpus.dec,testing.corpus.true)
#unigrams
test.dtm.unigrams<- DocumentTermMatrix(test.dtm,list(dictionary=dimnames(training.dtm.unigrams)[[2]]))
test.dtm.unigrams <- as.matrix(test.dtm.unigrams)
#bigrams
test.dtm.bigrams<- DocumentTermMatrix(test.dtm,list(dictionary=dimnames(training.dtm.bigrams)[[2]]))
test.dtm.bigrams <- as.matrix(test.dtm.bigrams)
#test set with both unigrams and bigrams
test.labels <- c(rep(0,80),rep(1,80))
test.labels <- as.factor(test.labels)
test.dtm <- cbind(test.dtm.unigrams, test.dtm.bigrams)
#######################################################
#with only unigrams
training.dtm.unigrams <- as.data.frame(training.dtm.unigrams)
test.dtm.unigrams <- as.data.frame(test.dtm.unigrams)
OOB.matrix.unigrams <- tuneRF(x =training.dtm.unigrams,
y = training.labels,
ntreeTry = 500, doBest = FALSE, plot = TRUE)
print(OOB.matrix.unigrams)
optimal.mtry.unigrams <- OOB.matrix.unigrams[which.min(OOB.matrix.unigrams[,2]),1]
print(optimal.mtry.unigrams)
classifier.unigrams <- randomForest(x = training.dtm.unigrams,
y = training.labels, ntree = 1000,
mtry = optimal.mtry.unigrams, type = "classification", err.rate = TRUE)
err.rate.unigrams <- cbind( c(1:1000),classifier.unigrams$err.rate[,1])
optimal.ntree.unigrams <- err.rate.unigrams[which.min(err.rate.unigrams[,2]), 1]
print(optimal.ntree.unigrams)
colnames(err.rate.unigrams) <- c( "ntree","OOB error")
plot(err.rate.unigrams)
classifier.unigrams <- randomForest(x = training.dtm.unigrams,
y = training.labels, mtry = optimal.mtry.unigrams,
ntree = optimal.ntree.unigrams, type = "classification")
# Predicting the Test set results
random.forests.predictions.unigrams <- predict(classifier.unigrams, newdata = test.dtm.unigrams)
print(table(random.forests.predictions.unigrams,test.labels))
############################################################
#with both unigrams and bigrams
training.dtm <- as.data.frame(training.dtm)
test.dtm <- as.data.frame(test.dtm)
OOB.matrix <- tuneRF(x =training.dtm,
y = training.labels,
ntreeTry = 500, doBest = FALSE, plot = TRUE)
print(OOB.matrix)
optimal.mtry <- OOB.matrix[which.min(OOB.matrix[,2]),1]
print(optimal.mtry)
classifier <- randomForest(x = training.dtm,
y = training.labels, ntree = 1000,
mtry = optimal.mtry, type = "classification", err.rate = TRUE)
err.rate <- cbind(c(1:1000),classifier$err.rate[,1] )
optimal.ntree <- err.rate[which.min(err.rate[,2]), 1]
print(optimal.ntree)
colnames(err.rate) <- c( "ntree","OOB error")
plot(err.rate)
classifier <- randomForest(x = training.dtm,
y = training.labels, mtry = optimal.mtry,
ntree = optimal.ntree, type = "classification")
# Predicting the Test set results
random.forests.predictions <- predict(classifier, newdata = test.dtm)
print(table(random.forests.predictions,test.labels))
library(tm)
library(entropy)
library(SnowballC)
library(rpart)
library(rpart.plot)
#training set
training.dtm <- cleaning.function(training.corpus.dec,training.corpus.true)
#extraction of unigrams
training.dtm.unigrams <- DocumentTermMatrix(training.dtm)
training.dtm.unigrams <- removeSparseTerms(training.dtm.unigrams,0.95)
training.dtm.unigrams <- as.matrix(training.dtm.unigrams)
#extraction of bigrams
BigramTokenizer <-function(x) unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
training.dtm.bigrams <- DocumentTermMatrix(training.dtm,control = list(tokenize = BigramTokenizer))
training.dtm.bigrams <- removeSparseTerms(training.dtm.bigrams,0.95)
training.dtm.bigrams <- as.matrix(training.dtm.bigrams)
#training set with both unigrams and bigrams
training.labels <- c(rep(0,320),rep(1,320))
training.dtm <- cbind(training.dtm.unigrams, training.dtm.bigrams)
###################################################################################
#test set
test.dtm <- cleaning.function (testing.corpus.dec,testing.corpus.true)
#unigrams
test.dtm.unigrams<- DocumentTermMatrix(test.dtm,list(dictionary=dimnames(training.dtm.unigrams)[[2]]))
test.dtm.unigrams <- as.matrix(test.dtm.unigrams)
#bigrams
test.dtm.bigrams<- DocumentTermMatrix(test.dtm,list(dictionary=dimnames(training.dtm.bigrams)[[2]]))
test.dtm.bigrams <- as.matrix(test.dtm.bigrams)
#test set with both unigrams and bigrams
test.labels <- c(rep(0,80),rep(1,80))
test.dtm <- cbind(test.dtm.unigrams, test.dtm.bigrams)
#######################################################
# grow the tree with only unigrams
reviews.rpart.unigrams <- rpart(label~.,
data=data.frame(training.dtm.unigrams,label = training.labels),
cp=0,method="class", minbucket = 1, minsplit = 2 )
# tree with lowest cv error
opt.cp.unigrams <-  reviews.rpart.unigrams$cptable[which.min(reviews.rpart.unigrams$cptable[,"xerror"]),"CP"]
print(opt.cp.unigrams)
plotcp(reviews.rpart.unigrams)
printcp(reviews.rpart.unigrams)
reviews.rpart.unigrams.pruned <- prune(reviews.rpart.unigrams,cp = reviews.rpart.unigrams$cptable[which.min(reviews.rpart.unigrams$cptable[,"xerror"]),"CP"] )
rpart.plot(reviews.rpart.unigrams.pruned, roundint = FALSE)
# make predictions on the test set
reviews.rpart.unigrams.pred <- predict(reviews.rpart.unigrams.pruned,
newdata=data.frame(as.matrix(test.dtm.unigrams)),type="class")
# show confusion matrix
print(table(reviews.rpart.unigrams.pred,test.labels))
#######################################################################################
#unigrams and bigrams
reviews.rpart <- rpart(label~.,
data=data.frame(training.dtm,label = training.labels),
cp=0,method="class", minbucket = 1, minsplit = 2)
# tree with lowest cv error
opt.cp <-  reviews.rpart$cptable[which.min(reviews.rpart$cptable[,"xerror"]),"CP"]
print(opt.cp)
plotcp(reviews.rpart)
printcp(reviews.rpart)
reviews.rpart.pruned <- prune(reviews.rpart,cp = reviews.rpart$cptable[which.min(reviews.rpart$cptable[,"xerror"]),"CP"] )
rpart.plot(reviews.rpart.unigrams.pruned, roundint = FALSE)
# make predictions on the test set
reviews.rpart.pred <- predict(reviews.rpart.pruned,
newdata=data.frame(as.matrix(test.dtm)),type="class")
# show confusion matrix
print(table(reviews.rpart.pred,test.labels))
library("glmnet")
#training set
training.dtm <- cleaning.function(training.corpus.dec,training.corpus.true)
#extraction of unigrams
training.dtm.unigrams <- DocumentTermMatrix(training.dtm)
training.dtm.unigrams <- removeSparseTerms(training.dtm.unigrams,0.95)
training.dtm.unigrams <- as.matrix(training.dtm.unigrams)
#extraction of bigrams
BigramTokenizer <-function(x) unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
training.dtm.bigrams <- DocumentTermMatrix(training.dtm,control = list(tokenize = BigramTokenizer))
training.dtm.bigrams <- removeSparseTerms(training.dtm.bigrams,0.95)
training.dtm.bigrams <- as.matrix(training.dtm.bigrams)
#training set with both unigrams and bigrams
training.labels <- c(rep(0,320),rep(1,320))
training.dtm <- cbind(training.dtm.unigrams, training.dtm.bigrams)
#test set
test.dtm <- cleaning.function (testing.corpus.dec,testing.corpus.true)
#unigrams
test.dtm.unigrams<- DocumentTermMatrix(test.dtm,list(dictionary=dimnames(training.dtm.unigrams)[[2]]))
test.dtm.unigrams <- as.matrix(test.dtm.unigrams)
#bigrams
test.dtm.bigrams<- DocumentTermMatrix(test.dtm,list(dictionary=dimnames(training.dtm.bigrams)[[2]]))
test.dtm.bigrams <- as.matrix(test.dtm.bigrams)
#test set with both unigrams and bigrams
test.labels <- c(rep(0,80),rep(1,80))
test.dtm <- cbind(test.dtm.unigrams, test.dtm.bigrams)
####################################################################
#first model (only unigrams)
reviews.glmnet.unigrams <- cv.glmnet(training.dtm.unigrams,training.labels,
family="binomial",type.measure="class")
print(coef(reviews.glmnet.unigrams,s="lambda.min"))
print(reviews.glmnet.unigrams$lambda.min)
plot (reviews.glmnet.unigrams)
reviews.logreg.pred.unigrams <- predict(reviews.glmnet.unigrams,
newx=test.dtm.unigrams,s="lambda.min",type="class")
print(table(reviews.logreg.pred.unigrams,test.labels))
#second model ( with bigrams)
reviews.glmnet <- cv.glmnet(training.dtm,training.labels,
family="binomial",type.measure="class")
print(coef(reviews.glmnet,s="lambda.min"))
print(reviews.glmnet$lambda.min)
plot(reviews.glmnet)
reviews.logreg.pred <- predict(reviews.glmnet,
newx=test.dtm,s="lambda.min",type="class")
print(table(reviews.logreg.pred.unigrams,test.labels))
source('~/GitHub/DataMining/assignment 2/code/Naive Bayes.R')
library(entropy)
#training set
training.dtm <- cleaning.function(training.corpus.dec,training.corpus.true)
#extraction of unigrams
training.dtm.unigrams <- DocumentTermMatrix(training.dtm)
training.dtm.unigrams <- removeSparseTerms(training.dtm.unigrams,0.95)
training.dtm.unigrams <- as.matrix(training.dtm.unigrams)
#extraction of bigrams
BigramTokenizer <-function(x) unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
training.dtm.bigrams <- DocumentTermMatrix(training.dtm,control = list(tokenize = BigramTokenizer))
training.dtm.bigrams <- removeSparseTerms(training.dtm.bigrams,0.95)
training.dtm.bigrams <- as.matrix(training.dtm.bigrams)
#training set with both unigrams and bigrams
training.labels <- c(rep(0,320),rep(1,320))
training.dtm <- cbind(training.dtm.unigrams, training.dtm.bigrams)
###################################################################################
#test set
test.dtm <- cleaning.function (testing.corpus.dec,testing.corpus.true)
#unigrams
test.dtm.unigrams<- DocumentTermMatrix(test.dtm,list(dictionary=dimnames(training.dtm.unigrams)[[2]]))
test.dtm.unigrams <- as.matrix(test.dtm.unigrams)
#bigrams
test.dtm.bigrams<- DocumentTermMatrix(test.dtm,list(dictionary=dimnames(training.dtm.bigrams)[[2]]))
test.dtm.bigrams <- as.matrix(test.dtm.bigrams)
#test set with both unigrams and bigrams
test.labels <- c(rep(0,80),rep(1,80))
test.dtm <- cbind(test.dtm.unigrams, test.dtm.bigrams)
########################################################################################
#feature selection (with mutual information, only for unigrams)
training.dtm.unigrams.mi <- apply(training.dtm.unigrams,2,function(x,y){
mi.plugin(table(x,y)/length(y))},training.labels)
training.dtm.unigrams.mi.order <- order(training.dtm.unigrams.mi,decreasing = T)
#feature selection ( with mutual information)
training.dtm.mi <- apply(training.dtm,2,function(x,y){
mi.plugin(table(x,y)/length(y))},training.labels)
training.dtm.mi.order <- order(training.dtm.mi,decreasing = T)
###########################################################################################
#print
print(dim(training.dtm.bigrams))
print(dim(training.dtm.unigrams))
print(colnames(training.dtm.bigrams))
print(dim(training.dtm)) ## just to check
print(training.dtm.mi[training.dtm.mi.order[1:10]])
#first model ( with feature selection according to mutual information)(only unigrams)
accuracies.unigrams.mi.models <- sapply(c(2:307), function (num.features){
model.mi <-train.mnb(training.dtm.unigrams[,training.dtm.unigrams.mi.order[1:num.features] ], training.labels)
predictions.mi <- predict.mnb(model.mi , test.dtm.unigrams[,training.dtm.unigrams.mi.order[1:num.features]])
conf.mat <- table (predictions.mi ,test.labels)
return (sum(diag(conf.mat))/180)
} )
accuracies.unigrams.mat <- cbind (accuracies.unigrams.mi.models, c(2:307))
accuracies.unigrams.best.n <- accuracies.unigrams.mat[which.max(accuracies.unigrams.mat[,1]), 2]
print(accuracies.unigrams.mat)
print(accuracies.unigrams.best.n)
plot(accuracies.unigrams.mat[,2] ,accuracies.unigrams.mat[,1], xlab = "n", ylab = "accuracy", type = "l")
model.unigrams.mi <-train.mnb(training.dtm.unigrams[,training.dtm.unigrams.mi.order[1:accuracies.unigrams.best.n] ], training.labels)
naive.bayes.predictions.unigrams.mi <- predict.mnb(model.unigrams.mi , test.dtm.unigrams[,training.dtm.unigrams.mi.order[1:accuracies.unigrams.best.n]])
print(table (naive.bayes.predictions.unigrams.mi ,test.labels))
#second model ( with feature selection according to mutual information)(both unigrams and bigrams)
accuracies.mi.models <- sapply(c(2:319), function (num.features){
model.mi <-train.mnb(training.dtm[,training.dtm.mi.order[1:num.features] ], training.labels)
predictions.mi <- predict.mnb(model.mi , test.dtm[,training.dtm.mi.order[1:num.features]])
conf.mat <- table (predictions.mi ,test.labels)
return (sum(diag(conf.mat))/180)
} )
accuracies.mat <- cbind (accuracies.mi.models, c(2:319))
accuracies.best <- accuracies.mat[which.max(accuracies.mat[,1]), 2]
print(accuracies.mat)
print(accuracies.best)
plot(accuracies.mat[,2] ,accuracies.mat[,1], xlab = "n", ylab = "accuracy", type = "l")
model.mi <-train.mnb(training.dtm[,training.dtm.mi.order[1:accuracies.best] ], training.labels)
naive.bayes.predictions.mi <- predict.mnb(model.mi , test.dtm[,training.dtm.mi.order[1:accuracies.best]])
print(table (naive.bayes.predictions.mi ,test.labels))
#training set
training.dtm <- cleaning.function(training.corpus.dec,training.corpus.true)
#extraction of unigrams
training.dtm.unigrams <- DocumentTermMatrix(training.dtm)
training.dtm.unigrams <- removeSparseTerms(training.dtm.unigrams,0.95)
training.dtm.unigrams <- as.matrix(training.dtm.unigrams)
#extraction of bigrams
BigramTokenizer <-function(x) unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)
training.dtm.bigrams <- DocumentTermMatrix(training.dtm,control = list(tokenize = BigramTokenizer))
training.dtm.bigrams <- removeSparseTerms(training.dtm.bigrams,0.95)
training.dtm.bigrams <- as.matrix(training.dtm.bigrams)
#training set with both unigrams and bigrams
training.labels <- c(rep(0,320),rep(1,320))
training.dtm <- cbind(training.dtm.unigrams, training.dtm.bigrams)
###################################################################################
#test set
test.dtm <- cleaning.function (testing.corpus.dec,testing.corpus.true)
#unigrams
test.dtm.unigrams<- DocumentTermMatrix(test.dtm,list(dictionary=dimnames(training.dtm.unigrams)[[2]]))
test.dtm.unigrams <- as.matrix(test.dtm.unigrams)
#bigrams
test.dtm.bigrams<- DocumentTermMatrix(test.dtm,list(dictionary=dimnames(training.dtm.bigrams)[[2]]))
test.dtm.bigrams <- as.matrix(test.dtm.bigrams)
#test set with both unigrams and bigrams
test.labels <- c(rep(0,80),rep(1,80))
test.dtm <- cbind(test.dtm.unigrams, test.dtm.bigrams)
########################################################################################
#feature selection (with mutual information, only for unigrams)
training.dtm.unigrams.mi <- apply(training.dtm.unigrams,2,function(x,y){
mi.plugin(table(x,y)/length(y))},training.labels)
training.dtm.unigrams.mi.order <- order(training.dtm.unigrams.mi,decreasing = T)
#feature selection ( with mutual information)
training.dtm.mi <- apply(training.dtm,2,function(x,y){
mi.plugin(table(x,y)/length(y))},training.labels)
training.dtm.mi.order <- order(training.dtm.mi,decreasing = T)
###########################################################################################
#print
print(dim(training.dtm.bigrams))
print(dim(training.dtm.unigrams))
print(colnames(training.dtm.bigrams))
print(dim(training.dtm)) ## just to check
print(training.dtm.mi[training.dtm.mi.order[1:10]])
#first model ( with feature selection according to mutual information)(only unigrams)
accuracies.unigrams.mi.models <- sapply(c(2:307), function (num.features){
model.mi <-train.mnb(training.dtm.unigrams[,training.dtm.unigrams.mi.order[1:num.features] ], training.labels)
predictions.mi <- predict.mnb(model.mi , test.dtm.unigrams[,training.dtm.unigrams.mi.order[1:num.features]])
conf.mat <- table (predictions.mi ,test.labels)
return (sum(diag(conf.mat))/180)
} )
accuracies.unigrams.mat <- cbind (accuracies.unigrams.mi.models, c(2:307))
accuracies.unigrams.best.n <- accuracies.unigrams.mat[which.max(accuracies.unigrams.mat[,1]), 2]
print(accuracies.unigrams.mat)
print(accuracies.unigrams.best.n)
plot(accuracies.unigrams.mat[,2] ,accuracies.unigrams.mat[,1], xlab = "n", ylab = "accuracy", type = "l")
model.unigrams.mi <-train.mnb(training.dtm.unigrams[,training.dtm.unigrams.mi.order[1:accuracies.unigrams.best.n] ], training.labels)
print(model.unigrams.mi)
naive.bayes.predictions.unigrams.mi <- predict.mnb(model.unigrams.mi , test.dtm.unigrams[,training.dtm.unigrams.mi.order[1:accuracies.unigrams.best.n]])
print(table (naive.bayes.predictions.unigrams.mi ,test.labels))
#second model ( with feature selection according to mutual information)(both unigrams and bigrams)
accuracies.mi.models <- sapply(c(2:319), function (num.features){
model.mi <-train.mnb(training.dtm[,training.dtm.mi.order[1:num.features] ], training.labels)
predictions.mi <- predict.mnb(model.mi , test.dtm[,training.dtm.mi.order[1:num.features]])
conf.mat <- table (predictions.mi ,test.labels)
return (sum(diag(conf.mat))/180)
} )
accuracies.mat <- cbind (accuracies.mi.models, c(2:319))
accuracies.best <- accuracies.mat[which.max(accuracies.mat[,1]), 2]
print(accuracies.mat)
print(accuracies.best)
plot(accuracies.mat[,2] ,accuracies.mat[,1], xlab = "n", ylab = "accuracy", type = "l")
model.mi <-train.mnb(training.dtm[,training.dtm.mi.order[1:accuracies.best] ], training.labels)
print(model.mi)
naive.bayes.predictions.mi <- predict.mnb(model.mi , test.dtm[,training.dtm.mi.order[1:accuracies.best]])
print(table (naive.bayes.predictions.mi ,test.labels))
